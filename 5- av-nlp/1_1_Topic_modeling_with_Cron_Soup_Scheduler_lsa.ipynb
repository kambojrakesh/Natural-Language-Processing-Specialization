{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_1_Topic_modeling_with_Cron_Soup_Scheduler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SAuZUwJJBgs",
        "outputId": "05e8ef2b-966a-442c-8b03-c7e9013d2e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.1.0\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests\n",
        "!pip install schedule\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import schedule\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sb\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sb\n",
        "\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "SGAFaO0bI2qi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagesToGet= 1\n",
        "upperframe=[]  \n",
        "submeu_li = []\n",
        "n_topics = 50\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "all_stopwords = nlp.Defaults.stop_words"
      ],
      "metadata": {
        "id": "3Jl_0GJZGYkD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Soup\n",
        "def get_soup(url1):\n",
        "\n",
        "  page=\"\"\n",
        "  #an exception might be thrown, so the code should be in a try-except block\n",
        "  try:\n",
        "      #use the browser to get the url. This is suspicious command that might blow up.\n",
        "      page=requests.get(url1)                             # this might throw an exception if something goes wrong.\n",
        "\n",
        "  except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url1)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "  \n",
        "  soup=BeautifulSoup(page.text,'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "0mP7iF3LJG3X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch news\n",
        "def get_news(): \n",
        "  url = \"https://www.npr.org/sections/news/\" \n",
        "  frame =[]\n",
        "  soup = get_soup(url)\n",
        "  links=soup.find_all(class_=\"item\")\n",
        "  \n",
        "  for j in links:\n",
        "    St = j.find(\"div\",attrs={'class':'item-info'}).find(\"h2\").text.strip()\n",
        "    La = j.find(\"p\").text.strip()\n",
        "    if(len(La) > 10 and len(St) > 10):\n",
        "      frame.append((St + \" \" +La))\n",
        "    \n",
        "  submeu=soup.find(class_=\"animated\")#.find_all(\"li\")\n",
        "\n",
        "  for a in submeu.find_all('a', href=True):\n",
        "    url = \"https://www.npr.org\" + a['href']\n",
        "    soup = get_soup(url)\n",
        "    links=soup.find_all(class_=\"item\")\n",
        "    for j in links:\n",
        "      Statement = j.find(\"div\",attrs={'class':'item-info'}).find(\"h2\").text.strip()\n",
        "      Label = j.find(\"p\").text.strip()\n",
        "      if(len(Label) > 5 and len(Statement) > 5):\n",
        "        frame.append((Statement+ \" \" +Label))\n",
        "\n",
        "  frame = [re.sub('\\n','', f) for f in frame]\n",
        "  frame = [re.sub(' +', ' ', f) for f in frame]\n",
        "  frame = list(set(frame))\n",
        "\n",
        "  return frame"
      ],
      "metadata": {
        "id": "aWeMN03uIKhK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "def remove_mystopwords(sentence):\n",
        "    tokens = sentence.split(\" \")\n",
        "    tokens_filtered= [word for word in tokens if not word in all_stopwords]\n",
        "    return (\" \").join(tokens_filtered)"
      ],
      "metadata": {
        "id": "PaE-r8OZI-uG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean data\n",
        "def data_cleaning(frame):\n",
        "  en_core = spacy.load('en_core_web_sm')\n",
        "  #print(\"++++++++\", frame)\n",
        "  lemma_frame_ls = []\n",
        "  frame = [f.lower() for f in frame]\n",
        "  frame = [re.sub('hide caption','', word) for word in frame]\n",
        "  frame = [re.sub('getty images','', word) for word in frame]\n",
        "  frame = [re.sub('[^A-Za-z ]+', '', word) for word in frame]\n",
        "  #print(\"--------------\", frame)\n",
        "  frame =[remove_mystopwords(text) for text in frame]\n",
        "  frame = [re.sub(' +', ' ', word) for word in frame]\n",
        "\n",
        "  df = pd.DataFrame(frame, columns=['data'])\n",
        "  df[\"data_cleanned\"] = df['data'].apply(lambda x: \" \".join([y.lemma_ for y in en_core(x)]))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "9vQlhUuPxdA-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    returns a tuple of the top n words in a sample and their \n",
        "    accompanying counts, given a CountVectorizer object and text sample\n",
        "    '''\n",
        "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
        "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
        "    \n",
        "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for \n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])"
      ],
      "metadata": {
        "id": "LB1KqXukcBJV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_keys(topic_matrix):\n",
        "    '''\n",
        "    returns an integer list of predicted topic \n",
        "    categories for a given topic matrix\n",
        "    '''\n",
        "    keys = topic_matrix.argmax(axis=1).tolist()\n",
        "    return keys\n",
        "\n",
        "def keys_to_counts(keys):\n",
        "    '''\n",
        "    returns a tuple of topic categories and their \n",
        "    accompanying magnitudes for a given list of keys\n",
        "    '''\n",
        "    count_pairs = Counter(keys).items()\n",
        "    categories = [pair[0] for pair in count_pairs]\n",
        "    counts = [pair[1] for pair in count_pairs]\n",
        "    return (categories, counts)"
      ],
      "metadata": {
        "id": "WtMIFykI1g_v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
        "    '''\n",
        "    returns a list of n_topic strings, where each string contains the n most common \n",
        "    words in a predicted category, in order\n",
        "    '''\n",
        "    top_word_indices = []\n",
        "    for topic in range(n_topics):\n",
        "      try:\n",
        "        temp_vector_sum = 0\n",
        "        for i in range(len(keys)):\n",
        "            if keys[i] == topic:\n",
        "                temp_vector_sum += document_term_matrix[i]\n",
        "        if not isinstance(temp_vector_sum, int):\n",
        "          temp_vector_sum = temp_vector_sum.toarray()\n",
        "          top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
        "          top_word_indices.append(top_n_word_indices) \n",
        "      except:\n",
        "        continue  \n",
        "    top_words = []\n",
        "    for topic in top_word_indices:\n",
        "        topic_words = []\n",
        "        for index in topic:\n",
        "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
        "            temp_word_vector[:,index] = 1\n",
        "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
        "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
        "        top_words.append(\" \".join(topic_words))         \n",
        "    return top_words"
      ],
      "metadata": {
        "id": "FqMkT7Ah2jTJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement LSA\n",
        "def impl_lsa():\n",
        "  reindexed_data = data_cleaning(get_news())\n",
        "  reindexed_data = reindexed_data['data_cleanned']\n",
        "  count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
        "\n",
        "  print('Headline before vectorization: {}'.format(reindexed_data[123]))\n",
        "\n",
        "  document_term_matrix = count_vectorizer.fit_transform(reindexed_data)\n",
        "\n",
        "  n_topics = 20\n",
        "  lsa_model = TruncatedSVD(n_components=n_topics)\n",
        "  lsa_topic_matrix = lsa_model.fit_transform(document_term_matrix)\n",
        "\n",
        "  lsa_keys = get_keys(lsa_topic_matrix)\n",
        "  lsa_categories, lsa_counts = keys_to_counts(lsa_keys)\n",
        "  top_3_words_lsa = get_top_n_words(3, lsa_keys, document_term_matrix, count_vectorizer)\n",
        "  #for t in range(n_topics):\n",
        "      #print(top_3_words_lda[t])\n",
        "  return top_3_words_lsa"
      ],
      "metadata": {
        "id": "MnUVyNqP23t5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Topic in CSV file\n",
        "def save_tiopic():\n",
        "  top_3_words_lsa = impl_lsa()\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H_%M_%S\")\n",
        "  print(\"Current thread execution time =\", current_time)\n",
        "  \n",
        "  \n",
        "  print(\"Current Topic: \", top_3_words_lsa)\n",
        "  df = pd.DataFrame(top_3_words_lsa)\n",
        "  df.to_csv(f\"a_{current_time}.csv\", index = False, header=True)"
      ],
      "metadata": {
        "id": "ZPbvzXYCPWrB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H_%M_%S\")\n",
        "  print(\"Current thread execution time =\", current_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZG9QrNp_sh7",
        "outputId": "44624c5a-8759-4ba6-b292-fb00d185882c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current thread execution time = 22_30_12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Scheduler\n",
        "def schedule_actions():\n",
        "  schedule.every().day.at('22:30:27').do(save_tiopic)\n",
        "  #schedule.every(1).minutes.do(save_tiopic)\n",
        "  while True:\n",
        "    schedule.run_pending()\n",
        "    sleep(1)\n",
        "\n",
        "schedule_actions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "KSctqA5e04dD",
        "outputId": "5d683874-d171-427d-d224-d071efdaad44"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headline before vectorization: man charge kill mother sea inherit family estate nathan carman arrive small boat coast guard station boston sept spending week sea life raft rescue michael dwyerap\n",
            "Current thread execution time = 22_30_33\n",
            "Current Topic:  ['abortion court supreme', 'ukraine ukrainian climate', 'new york covid', 'noose university say', 'primary ohio rep', 'press secretary jeanpierre', 'climate black year', 'oil russian sanction', 'texas abortion law', 'al kill jazeera', 'photo astronaut april', 'video sheriff game', 'gas soon export', 'covid big global', 'texas oyster mason', 'southside people school', 'drug die age', 'marcos presidential hari', 'human scientist emotion', 'solar project nasa']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-18c137b6c59d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mschedule_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-18c137b6c59d>\u001b[0m in \u001b[0;36mschedule_actions\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mschedule_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tf8DOqiD-gff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}