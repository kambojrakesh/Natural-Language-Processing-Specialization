{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_1_Topic_modeling_with_Cron_Soup_Scheduler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SAuZUwJJBgs",
        "outputId": "757b939b-69cb-4967-b6b2-b6171c3aebb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests\n",
        "!pip install schedule\n",
        "!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import schedule\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sb\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "from bokeh.models import Label\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import seaborn as sb\n",
        "\n",
        "from textblob import TextBlob\n",
        "import scipy.stats as stats\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "SGAFaO0bI2qi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagesToGet= 1\n",
        "upperframe=[]  \n",
        "submeu_li = []\n",
        "n_topics = 50\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "all_stopwords = nlp.Defaults.stop_words"
      ],
      "metadata": {
        "id": "3Jl_0GJZGYkD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Soup\n",
        "def get_soup(url1):\n",
        "\n",
        "  page=\"\"\n",
        "  #an exception might be thrown, so the code should be in a try-except block\n",
        "  try:\n",
        "      #use the browser to get the url. This is suspicious command that might blow up.\n",
        "      page=requests.get(url1)                             # this might throw an exception if something goes wrong.\n",
        "\n",
        "  except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url1)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "  \n",
        "  soup=BeautifulSoup(page.text,'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "0mP7iF3LJG3X"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch news\n",
        "def get_news(): \n",
        "  url = \"https://www.npr.org/sections/news/\" \n",
        "  frame =[]\n",
        "  soup = get_soup(url)\n",
        "  links=soup.find_all(class_=\"item\")\n",
        "  \n",
        "  for j in links:\n",
        "    St = j.find(\"div\",attrs={'class':'item-info'}).find(\"h2\").text.strip()\n",
        "    La = j.find(\"p\").text.strip()\n",
        "    if(len(La) > 10 and len(St) > 10):\n",
        "      frame.append((St + \" \" +La))\n",
        "    \n",
        "  submeu=soup.find(class_=\"animated\")#.find_all(\"li\")\n",
        "\n",
        "  for a in submeu.find_all('a', href=True):\n",
        "    url = \"https://www.npr.org\" + a['href']\n",
        "    soup = get_soup(url)\n",
        "    links=soup.find_all(class_=\"item\")\n",
        "    for j in links:\n",
        "      Statement = j.find(\"div\",attrs={'class':'item-info'}).find(\"h2\").text.strip()\n",
        "      Label = j.find(\"p\").text.strip()\n",
        "      if(len(Label) > 5 and len(Statement) > 5):\n",
        "        frame.append((Statement+ \" \" +Label))\n",
        "\n",
        "  frame = [re.sub('\\n','', f) for f in frame]\n",
        "  frame = [re.sub(' +', ' ', f) for f in frame]\n",
        "  frame = list(set(frame))\n",
        "\n",
        "  return frame"
      ],
      "metadata": {
        "id": "aWeMN03uIKhK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "def remove_mystopwords(sentence):\n",
        "    tokens = sentence.split(\" \")\n",
        "    tokens_filtered= [word for word in tokens if not word in all_stopwords]\n",
        "    return (\" \").join(tokens_filtered)"
      ],
      "metadata": {
        "id": "PaE-r8OZI-uG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean data\n",
        "def data_cleaning(frame):\n",
        "  en_core = spacy.load('en_core_web_sm')\n",
        "  #print(\"++++++++\", frame)\n",
        "  lemma_frame_ls = []\n",
        "  frame = [f.lower() for f in frame]\n",
        "  frame = [re.sub('hide caption','', word) for word in frame]\n",
        "  frame = [re.sub('getty images','', word) for word in frame]\n",
        "  frame = [re.sub('[^A-Za-z ]+', '', word) for word in frame]\n",
        "  #print(\"--------------\", frame)\n",
        "  frame =[remove_mystopwords(text) for text in frame]\n",
        "  frame = [re.sub(' +', ' ', word) for word in frame]\n",
        "\n",
        "  df = pd.DataFrame(frame, columns=['data'])\n",
        "  df[\"data_cleanned\"] = df['data'].apply(lambda x: \" \".join([y.lemma_ for y in en_core(x)]))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "9vQlhUuPxdA-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    returns a tuple of the top n words in a sample and their \n",
        "    accompanying counts, given a CountVectorizer object and text sample\n",
        "    '''\n",
        "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
        "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
        "    \n",
        "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for \n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])"
      ],
      "metadata": {
        "id": "LB1KqXukcBJV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_keys(topic_matrix):\n",
        "    '''\n",
        "    returns an integer list of predicted topic \n",
        "    categories for a given topic matrix\n",
        "    '''\n",
        "    keys = topic_matrix.argmax(axis=1).tolist()\n",
        "    return keys\n",
        "\n",
        "def keys_to_counts(keys):\n",
        "    '''\n",
        "    returns a tuple of topic categories and their \n",
        "    accompanying magnitudes for a given list of keys\n",
        "    '''\n",
        "    count_pairs = Counter(keys).items()\n",
        "    categories = [pair[0] for pair in count_pairs]\n",
        "    counts = [pair[1] for pair in count_pairs]\n",
        "    return (categories, counts)"
      ],
      "metadata": {
        "id": "WtMIFykI1g_v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
        "    '''\n",
        "    returns a list of n_topic strings, where each string contains the n most common \n",
        "    words in a predicted category, in order\n",
        "    '''\n",
        "    top_word_indices = []\n",
        "    for topic in range(n_topics):\n",
        "      try:\n",
        "        temp_vector_sum = 0\n",
        "        for i in range(len(keys)):\n",
        "            if keys[i] == topic:\n",
        "                temp_vector_sum += document_term_matrix[i]\n",
        "        if not isinstance(temp_vector_sum, int):\n",
        "          temp_vector_sum = temp_vector_sum.toarray()\n",
        "          top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
        "          top_word_indices.append(top_n_word_indices) \n",
        "      except:\n",
        "        continue  \n",
        "    top_words = []\n",
        "    for topic in top_word_indices:\n",
        "        topic_words = []\n",
        "        for index in topic:\n",
        "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
        "            temp_word_vector[:,index] = 1\n",
        "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
        "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
        "        top_words.append(\" \".join(topic_words))         \n",
        "    return top_words"
      ],
      "metadata": {
        "id": "FqMkT7Ah2jTJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement LDA\n",
        "def impl_lda():\n",
        "  reindexed_data = data_cleaning(get_news())\n",
        "  reindexed_data = reindexed_data['data_cleanned']\n",
        "  count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
        "\n",
        "  print('Headline before vectorization: {}'.format(reindexed_data[123]))\n",
        "\n",
        "  document_term_matrix = count_vectorizer.fit_transform(reindexed_data)\n",
        "\n",
        "  lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', \n",
        "                                            random_state=0, verbose=0)\n",
        "  lda_topic_matrix = lda_model.fit_transform(document_term_matrix)\n",
        "\n",
        "  lda_keys = get_keys(lda_topic_matrix)\n",
        "  lda_categories, lda_counts = keys_to_counts(lda_keys)\n",
        "  top_3_words_lda = get_top_n_words(3, lda_keys, document_term_matrix, count_vectorizer)\n",
        "  #for t in range(n_topics):\n",
        "      #print(top_3_words_lda[t])\n",
        "  return top_3_words_lda"
      ],
      "metadata": {
        "id": "MnUVyNqP23t5"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save Topic in CSV file\n",
        "def save_tiopic():\n",
        "  top_3_words_lda = impl_lda()\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H_%M_%S\")\n",
        "  print(\"Current thread execution time =\", current_time)\n",
        "  \n",
        "  \n",
        "  print(\"Current Topic: \", top_3_words_lda)\n",
        "  df = pd.DataFrame(top_3_words_lda)\n",
        "  df.to_csv(f\"a_{current_time}.csv\", index = False, header=True)"
      ],
      "metadata": {
        "id": "5n3QpmNe1iTx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H_%M_%S\")\n",
        "  print(\"Current thread execution time =\", current_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZG9QrNp_sh7",
        "outputId": "6c872386-c977-46aa-96cc-7c6339665569"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current thread execution time = 06_23_36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Scheduler\n",
        "def schedule_actions():\n",
        "  schedule.every().day.at('06:11:27').do(save_tiopic)\n",
        "  #schedule.every(1).minutes.do(save_tiopic)\n",
        "  while True:\n",
        "    schedule.run_pending()\n",
        "    sleep(1)\n",
        "\n",
        "schedule_actions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "KSctqA5e04dD",
        "outputId": "1862bb8c-69e6-4f76-a25d-aaad52a74325"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_05_22\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_06_20\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_06_27\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_07_26\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_07_32\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_08_31\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_08_38\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_09_37\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n",
            "Headline before vectorization: biden ask congress billion aid ukraine war drag president joe biden speak war ukraine roosevelt room white house thursday andrew harnikap\n",
            "Current thread execution time = 06_09_44\n",
            "Current Topic:  ['oyster texas johny', 'figure dance black', 'donnella summer demby', 'lgbtq youth care', 'australian porpoise minister', 'astronaut mariupol azovstal', 'sample state clot', 'senate ohio primary', 'kasha fairfax hear', 'code abortion hari', 'quantitative uranus tightening', 'texas solar fossil', 'detect test ba', 'new european eu', 'climate scotland dam', 'climate air carbon', 'president rep biden', 'ohio primary senate', 'mccarthy vaccine house', 'climate food change', 'pron emission perform', 'abortion supreme court', 'arizona fit judge', 'trump republican speak', 'shirt wear hold', 'twin alex spektor', 'gas harris april', 'bassist carter breed', 'internet npr player', 'mayorkas secretary dhs', 'new democratic variant', 'zurich gas chicago', 'new india million', 'biden capitol kesler', 'stock new york', 'water lake drinking', 'emotion health strike', 'knot influencer book', 'tulsa attack american', 'roe wade court', 'hospital johnson kira', 'library book card', 'new york free', 'employee playlist gap', 'oil russian gabrielle', 'climate change wave']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-745cfe78a7db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mschedule_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-745cfe78a7db>\u001b[0m in \u001b[0;36mschedule_actions\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mschedule_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Tf8DOqiD-gff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}