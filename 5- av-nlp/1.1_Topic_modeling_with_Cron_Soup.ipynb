{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic_modeling_with_Cron_Soup.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SAuZUwJJBgs",
        "outputId": "12fcfec2-6911-475f-a52b-4377128b44c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests\n",
        "!pip install schedule"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request,sys,time\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "import schedule\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n"
      ],
      "metadata": {
        "id": "SGAFaO0bI2qi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_top_words = 50 \n",
        "pagesToGet= 1\n",
        "upperframe=[]  \n",
        "submeu_li = []\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "all_stopwords = nlp.Defaults.stop_words"
      ],
      "metadata": {
        "id": "3Jl_0GJZGYkD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup(url1):\n",
        "\n",
        "  page=\"\"\n",
        "  #an exception might be thrown, so the code should be in a try-except block\n",
        "  try:\n",
        "      #use the browser to get the url. This is suspicious command that might blow up.\n",
        "      page=requests.get(url1)                             # this might throw an exception if something goes wrong.\n",
        "\n",
        "  except Exception as e:                                   # this describes what to do if an exception is thrown\n",
        "      error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
        "      print ('ERROR FOR LINK:',url1)                          #print the link that cause the problem\n",
        "      print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
        "  \n",
        "  soup=BeautifulSoup(page.text,'html.parser')\n",
        "  return soup"
      ],
      "metadata": {
        "id": "0mP7iF3LJG3X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_news(): \n",
        "  url = \"https://www.npr.org/sections/news/\" \n",
        "  frame =[]\n",
        "  soup = get_soup(url)\n",
        "  links=soup.find_all(class_=\"item\")\n",
        "  \n",
        "  for j in links:\n",
        "    St = j.find(\"div\",attrs={'class':'item-info'}).find(\"h2\").text.strip()\n",
        "    La = j.find(\"p\").text.strip()\n",
        "    if(len(La) > 20 and len(St) > 20):\n",
        "      frame.append((St + \" \" +La))\n",
        "    \n",
        "  submeu=soup.find(class_=\"animated\")#.find_all(\"li\")\n",
        "\n",
        "  for a in submeu.find_all('a', href=True):\n",
        "    url = \"https://www.npr.org\" + a['href']\n",
        "    soup = get_soup(url)\n",
        "    links=soup.find_all(class_=\"item\")\n",
        "    for j in links:\n",
        "      Statement = j.find(\"div\",attrs={'class':'item-info'}).find(\"h2\").text.strip()\n",
        "      Label = j.find(\"p\").text.strip()\n",
        "      if(len(Label) > 20 and len(Statement) > 20):\n",
        "        frame.append((Statement+ \" \" +Label))\n",
        "\n",
        "  frame = [re.sub('\\n','', f) for f in frame]\n",
        "  frame = [re.sub(' +', ' ', f) for f in frame]\n",
        "  frame = list(set(frame))\n",
        "  return frame"
      ],
      "metadata": {
        "id": "aWeMN03uIKhK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mystopwords(sentence):\n",
        "    tokens = sentence.split(\" \")\n",
        "    tokens_filtered= [word for word in tokens if not word in all_stopwords]\n",
        "    return (\" \").join(tokens_filtered)"
      ],
      "metadata": {
        "id": "PaE-r8OZI-uG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_cleaning(frame):\n",
        "  lemma_frame_ls = []\n",
        "  frame = [f.lower() for f in frame]\n",
        "  frame = [re.sub('hide caption','', word) for word in frame]\n",
        "  frame = [re.sub('getty images','', word) for word in frame]\n",
        "  frame = [re.sub('[^A-Za-z ]+', '', word) for word in frame]\n",
        "\n",
        "  frame =[remove_mystopwords(text) for text in frame]\n",
        "  frame = ' '.join(frame)\n",
        "  doc=nlp(frame)\n",
        "  # Lemmatizing the text\n",
        "  lemma_frame = [token.lemma_ for token in doc]\n",
        "  lemma_frame_str = ' '.join(lemma_frame)\n",
        "\n",
        "  lemma_frame_ls.append(lemma_frame_str)\n",
        "  frame = [re.sub(' +', ' ', word) for word in lemma_frame_ls]\n",
        "  return frame"
      ],
      "metadata": {
        "id": "9vQlhUuPxdA-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impl_lda():\n",
        "  from datetime import datetime\n",
        "  now = datetime.now()\n",
        "  current_time = now.strftime(\"%H_%M_%S\")\n",
        "  print(\"Current thread execution time =\", current_time)\n",
        "\n",
        "  frame = data_cleaning(get_news())\n",
        "  cv = CountVectorizer(stop_words='english', ngram_range=(3, 3))\n",
        "  cvz = cv.fit_transform(frame)\n",
        "  vocab = cv.get_feature_names_out()\n",
        "\n",
        "  lda = LatentDirichletAllocation(n_components=1, max_iter = 5000, random_state=20)\n",
        "  X_topics = lda.fit_transform(cvz)\n",
        "  topic_words = lda.components_\n",
        "\n",
        "  for i, topic_dist in enumerate(topic_words):\n",
        "      sorted_topic_dist = np.argsort(topic_dist)\n",
        "      topic_words = np.array(vocab)[sorted_topic_dist]\n",
        "      topic_words = topic_words[:-n_top_words:-1]\n",
        "      #print (\"Topic\", str(i+1), topic_words)\n",
        "\n",
        "  print(\"Current Topic: \", topic_words)\n",
        "  df = pd.DataFrame(topic_words)\n",
        "  df.to_csv(f\"a_{current_time}.csv\", index = False, header=True)"
      ],
      "metadata": {
        "id": "teCd2d2HhfQo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def schedule_actions():\n",
        "  schedule.every().day.at('12:00:00').do(impl_lda)\n",
        "  #schedule.every(2).minutes.do(impl_lda)\n",
        "  while True:\n",
        "    schedule.run_pending()\n",
        "    sleep(1)\n",
        "\n",
        "schedule_actions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "ekWFjjG7E5cd",
        "outputId": "9b85c364-a9a5-42a1-a406-a5369eeb14a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current thread execution time = 08_03_41\n",
            "Current Topic:  ['new york city' 'incandescent light bulb' 'russias invasion ukraine'\n",
            " 'tesla share sink' 'southern california resident'\n",
            " 'marjorie taylor greene' 'leader kevin mccarthy' 'tesla ceo elon'\n",
            " 'minority leader kevin' 'change climate change' 'house minority leader'\n",
            " 'demby summer thomad' 'great barrier reef' 'taylor greene testify'\n",
            " 'russiaukraine war happen' 'russian president vladimir'\n",
            " 'intercept russian military' 'story air atc' 'ceo elon musk'\n",
            " 'april rally scott' 'new movement stand' 'trump speak rally'\n",
            " 'president donald trump' 'ice shelf size' 'climate change climate'\n",
            " 'illegal voter registration' 'endorsement trump april'\n",
            " 'climate scientist say' 'gene demby summer' 'thomad leah donnella'\n",
            " 'americans live area' 'kevin mccarthy rcalif' 'elon musk speak'\n",
            " 'ohio draw angerer' 'rally scott olson' 'karen grigsby bates'\n",
            " 'trump april rally' 'delaware ohio draw' 'shelf size new'\n",
            " 'war happen today' 'size new york' 'movement stand rock'\n",
            " 'drug overdose death' 'australias great barrier' 'health care worker'\n",
            " 'donald trump speak' 'summer thomad leah' 'happen today april'\n",
            " 'rule federal judge']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-4cd63a9462c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mschedule_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-4cd63a9462c2>\u001b[0m in \u001b[0;36mschedule_actions\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mschedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pending\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mschedule_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}